<div align="center">

# üì∞ Keep Up Daily

**A free, automated, daily-curated tech feed for developers who never stop learning.**

[![Daily Feed](https://github.com/LuizMacedo/keep-up-daily/actions/workflows/deploy.yml/badge.svg)](https://github.com/LuizMacedo/keep-up-daily/actions/workflows/deploy.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![GitHub Pages](https://img.shields.io/badge/dashboard-live-brightgreen)](https://luizmacedo.github.io/keep-up-daily/)

[**üìñ Read the Dashboard ‚Üí**](https://luizmacedo.github.io/keep-up-daily/)

</div>

---

## What is this?

**Keep Up Daily** is a Python-powered web scraper that automatically collects trending tech articles, tutorials, and repositories every single day ‚Äî and publishes them as a clean, distraction-free reading dashboard on GitHub Pages.

No ads. No tracking. No monetization. Just pure learning content.

### Sources

| Source              | Type          | What it captures                         |
| ------------------- | ------------- | ---------------------------------------- |
| **Dev.to**          | REST API      | Top community articles                   |
| **Hacker News**     | Firebase API  | Top stories                              |
| **GitHub Trending** | HTML scraping | Daily trending repositories              |
| **Reddit**          | JSON API      | Hot posts from 11 programming subreddits |
| **Lobste.rs**       | JSON API      | Hottest tech stories                     |
| **Hashnode**        | GraphQL API   | Best developer blog posts                |

### Categories

Content is automatically categorized into: **AI & ML**, **Web Dev**, **DevOps & Cloud**, **Languages**, **Frameworks**, **Security**, **Career**, and **General**.

---

## Dashboard Features

- **Clean, focused reading experience** ‚Äî zero distractions, content-first design
- **Category filter tabs** ‚Äî focus on what matters to you
- **Full-text search** ‚Äî instantly filter across all articles
- **Date navigation** ‚Äî browse up to 30 days of history
- **Dark mode** ‚Äî respects your system preference, toggleable
- **Bilingual UI** ‚Äî English üá∫üá∏ and Portuguese üáßüá∑
- **Fully responsive** ‚Äî works on desktop, tablet, and mobile
- **No JavaScript frameworks** ‚Äî fast, lightweight vanilla JS + TailwindCSS

---

## Project Structure

```
keep-up-daily/
‚îú‚îÄ‚îÄ .github/workflows/
‚îÇ   ‚îî‚îÄ‚îÄ deploy.yml          # Daily cron + GitHub Pages deployment
‚îú‚îÄ‚îÄ scraper/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ __main__.py          # python -m scraper entrypoint
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # All configuration in one place
‚îÇ   ‚îú‚îÄ‚îÄ categorizer.py       # Keyword-based categorization
‚îÇ   ‚îú‚îÄ‚îÄ output.py            # JSON output + data retention
‚îÇ   ‚îî‚îÄ‚îÄ sources/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py      # Exports ALL_SCRAPERS list
‚îÇ       ‚îú‚îÄ‚îÄ base.py          # Article dataclass + BaseScraper
‚îÇ       ‚îú‚îÄ‚îÄ devto.py
‚îÇ       ‚îú‚îÄ‚îÄ hackernews.py
‚îÇ       ‚îú‚îÄ‚îÄ github_trending.py
‚îÇ       ‚îú‚îÄ‚îÄ reddit.py
‚îÇ       ‚îú‚îÄ‚îÄ lobsters.py
‚îÇ       ‚îî‚îÄ‚îÄ hashnode.py
‚îú‚îÄ‚îÄ web/
‚îÇ   ‚îú‚îÄ‚îÄ index.html           # Dashboard (TailwindCSS)
‚îÇ   ‚îî‚îÄ‚îÄ app.js               # Vanilla JS app logic + i18n
‚îú‚îÄ‚îÄ data/                    # Auto-generated JSON (one file per day)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
```

---

## Getting Started

### Prerequisites

- Python 3.10+
- pip

### Run Locally

```bash
# Clone the repository
git clone https://github.com/LuizMacedo/keep-up-daily.git
cd keep-up-daily

# Install dependencies
pip install -r requirements.txt

# Run the scraper
python -m scraper

# Output will be in data/YYYY-MM-DD.json
```

To preview the dashboard locally, serve the `web/` folder plus `data/`:

```bash
# Quick local preview
mkdir -p /tmp/kud-preview/data
cp web/* /tmp/kud-preview/
cp data/*.json /tmp/kud-preview/data/ 2>/dev/null
cd /tmp/kud-preview && python -m http.server 8000
# Open http://localhost:8000
```

### Automated (via GitHub Actions)

Once pushed to GitHub, the workflow runs automatically:

- **Every day at 8:00 AM Central Time (14:00 UTC)**
- Can also be triggered manually: _Actions ‚Üí Keep Up Daily ‚Üí Run workflow_
- Scrapes all sources, commits data, and deploys to GitHub Pages

---

## Enabling GitHub Pages

To activate the dashboard on your fork:

1. Go to your repository **Settings** ‚Üí **Pages**
2. Under **Build and deployment**, set **Source** to **GitHub Actions**
3. Trigger the workflow manually (Actions ‚Üí Keep Up Daily ‚Üí Run workflow)
4. Your dashboard will be live at `https://<your-username>.github.io/keep-up-daily/`

> **Important:** You must select **"GitHub Actions"** as the source, not "Deploy from a branch".

---

## How to Fork & Personalize

This project is designed to be forked and customized. Here's how:

### 1. Fork the repository

Click the **Fork** button at the top of this page.

### 2. Enable GitHub Pages

Follow the instructions above to set the source to "GitHub Actions".

### 3. Customize sources

Edit `scraper/config.py`:

```python
# Add/remove subreddits
SOURCES["reddit"]["subreddits"] = ["programming", "rust", "your_favorite_sub"]

# Disable a source
SOURCES["hashnode"]["enabled"] = False

# Add more GitHub Trending languages
SOURCES["github_trending"]["languages"] = ["", "python", "rust", "zig"]
```

### 4. Customize categories

Edit the `CATEGORIES` dictionary in `scraper/config.py` to add keywords for topics you care about.

### 5. Add a new source

Create a new file in `scraper/sources/` following this pattern:

```python
# scraper/sources/my_source.py
from .base import Article, BaseScraper

class MySourceScraper(BaseScraper):
    name = "my_source"

    def fetch(self) -> list[Article]:
        articles = []
        # Your scraping logic here...
        # Return a list of Article objects
        return articles
```

Then register it in `scraper/sources/__init__.py`:

```python
from .my_source import MySourceScraper

ALL_SCRAPERS = [
    # ...existing scrapers...
    MySourceScraper,
]
```

---

## Contributing

Contributions are welcome! This project is meant to help the developer community learn and grow.

### For New Contributors

1. **Fork** the repository
2. Create a feature branch: `git checkout -b feature/my-improvement`
3. Make your changes
4. Run the scraper locally to test: `python -m scraper`
5. Commit with a clear message: `git commit -m "Add: new source scraper for XYZ"`
6. Push and open a **Pull Request**

### Ideas for Contributions

- üåê **New sources** ‚Äî add scrapers for tech newsletters, podcasts, YouTube channels
- üè∑Ô∏è **Better categorization** ‚Äî improve keyword matching or add ML-based classification
- üåç **More languages** ‚Äî add UI translations beyond EN/PT
- üìä **Analytics** ‚Äî weekly trends, most popular topics, source comparison
- üé® **UI improvements** ‚Äî accessibility, animations, reading progress
- üì± **PWA support** ‚Äî offline reading, push notifications
- üß™ **Tests** ‚Äî unit tests for scrapers and categorizer

### For Experienced Developers

The codebase follows a modular architecture. Each source scraper is independent and follows the `BaseScraper` interface. The `Article` dataclass is the normalized schema. Feel free to refactor, optimize, or propose architectural changes.

---

## How It Works

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Dev.to     ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ  Categorize ‚îÇ
‚îÇ  HackerNews ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Orchestrator‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Deduplicate‚îÇ
‚îÇ  GitHub     ‚îÇ     ‚îÇ  (main.py)   ‚îÇ     ‚îÇ  Sort       ‚îÇ
‚îÇ  Reddit     ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ             ‚îÇ
‚îÇ  Lobsters   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  Hashnode   ‚îÇ                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                  ‚ñº
                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                          ‚îÇ data/       ‚îÇ
                                          ‚îÇ YYYY-MM-DD  ‚îÇ
                                          ‚îÇ .json       ‚îÇ
                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                 ‚îÇ
                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                          ‚îÇ GitHub Pages‚îÇ
                                          ‚îÇ Dashboard   ‚îÇ
                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Daily at 14:00 UTC**, GitHub Actions:

1. Runs all scrapers in sequence (with error isolation)
2. Deduplicates articles by URL
3. Categorizes using keyword matching
4. Sorts by engagement score
5. Saves to `data/YYYY-MM-DD.json`
6. Commits and pushes data
7. Builds and deploys the static dashboard

---

## Philosophy

> "The best time to start learning was yesterday. The second best time is today."

This project exists because:

- **Staying current** in tech is overwhelming ‚Äî there are too many sources to check
- **Curated feeds** shouldn't cost money or track you
- **Open source tools** should make learning accessible to everyone
- **Community-driven** means everyone benefits from everyone's contributions

---

## License

MIT ‚Äî use it however you want. See [LICENSE](LICENSE) for details.

---

<div align="center">

**Made with ‚ù§Ô∏è for the developer community**

_If this helps you learn something new, consider sharing it with a fellow developer._

‚≠ê Star this repo if you find it useful!

</div>
